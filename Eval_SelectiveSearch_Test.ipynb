{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/Samsung_T5/Haak_data/Flight1/Haak1_true_annot.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f52d008421d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mHaak1_true_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/Volumes/Samsung_T5/Haak_data/Flight1/Haak1_true_annot.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mHaak1_anomalies_object_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/Volumes/Samsung_T5/Haak_data/Flight1/Haak1_ano_object_annot.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mHaak1_anomalies_land_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/Volumes/Samsung_T5/Haak_data/Flight1/Haak1_ano_land_annot.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mHaak3_true_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/Volumes/Samsung_T5/Haak_data/Flight3/Haak3_true_annot.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/Samsung_T5/Haak_data/Flight1/Haak1_true_annot.csv'"
     ]
    }
   ],
   "source": [
    "Haak1_true_df = pd.read_csv(r'/Volumes/Samsung_T5/Haak_data/Flight1/Haak1_true_annot.csv')\n",
    "Haak1_anomalies_object_df = pd.read_csv(r'/Volumes/Samsung_T5/Haak_data/Flight1/Haak1_ano_object_annot.csv')\n",
    "Haak1_anomalies_land_df = pd.read_csv(r'/Volumes/Samsung_T5/Haak_data/Flight1/Haak1_ano_land_annot.csv')\n",
    "\n",
    "Haak3_true_df = pd.read_csv(r'/Volumes/Samsung_T5/Haak_data/Flight3/Haak3_true_annot.csv')\n",
    "Haak3_anomalies_object_df = pd.read_csv(r'/Volumes/Samsung_T5/Haak_data/Flight3/Haak3_ano_object_annot.csv')\n",
    "Haak3_anomalies_land_df = pd.read_csv(r'/Volumes/Samsung_T5/Haak_data/Flight3/Haak3_ano_land_annot.csv')\n",
    "\n",
    "Danilo_true_df = pd.read_csv(r'/Volumes/Samsung_T5/Danilo_data/IR/Danilo_true_annot.csv')\n",
    "Danilo_anomalies_land_df = pd.read_csv(r'/Volumes/Samsung_T5/Danilo_data/IR/Danilo_ano_land_annot.csv')\n",
    "Empty_df = pd.read_csv(r'/Volumes/Samsung_T5/Danilo_data/IR/Empty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Haak1_true_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4ef4cde75dce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Haak - Flight 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mHaak1_true_train\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mHaak1_true_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m69\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mHaak1_true_test\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mHaak1_true_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHaak1_true_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mHaak1_ano_object_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHaak1_anomalies_object_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m69\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Haak1_true_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Haak - Flight 1\n",
    "Haak1_true_train       = Haak1_true_df.sample(frac = 0.7, random_state = 100)\n",
    "Haak1_true_test        = Haak1_true_df.drop(Haak1_true_train.index)\n",
    "\n",
    "Haak1_ano_object_train = Haak1_anomalies_object_df.sample(frac = 0.7, random_state = 100)\n",
    "Haak1_ano_object_test  = Haak1_anomalies_object_df.drop(Haak1_ano_object_train.index)\n",
    "\n",
    "Haak1_ano_land_train   = Haak1_anomalies_land_df.sample(frac = 0.7, random_state = 100)\n",
    "Haak1_ano_land_test    = Haak1_anomalies_land_df.drop(Haak1_ano_land_train.index)\n",
    " \n",
    "# Haak - Flight 1\n",
    "Haak3_true_test        = Haak3_true_df\n",
    "Haak3_ano_object_test  = Haak3_anomalies_object_df\n",
    "Haak3_ano_land_test    = Haak3_anomalies_land_df\n",
    "\n",
    "# Danilo\n",
    "Danilo_true_test       = Danilo_true_df\n",
    "Danilo_ano_land_test   = Danilo_anomalies_land_df\n",
    "empty                  = Empty_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelx = np.array([[1,1,1], [0,0,0], [-1,-1,-1]])\n",
    "kernely = np.array([[-1,0,1],[-1,0,1],[-1,0,1]])\n",
    "\n",
    "# GAUSSIAN BLUR\n",
    "img_gaussian = cv2.GaussianBlur(img,(gaussian_mask, gaussian_mask), 0)\n",
    "\n",
    "# PREWITT\n",
    "img_prewitt_x = np.array(cv2.filter2D(img_gaussian, -1, kernelx))\n",
    "img_prewitt_y = np.array(cv2.filter2D(img_gaussian, -1, kernely))\n",
    "img_prewitt_z = np.array(img_prewitt_x + img_prewitt_y)\n",
    "\n",
    "# THRESHOLD\n",
    "img_prewitt_z_thrsh = cv2.threshold(img_prewitt_z,threshold_mask,255,cv2.THRESH_BINARY)[1]\n",
    "mask = img_prewitt_z_thrsh\n",
    "\n",
    "return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Helper-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covering_rectangle_from_square_boxes(boxes):\n",
    "    output = np.array([])\n",
    "    \n",
    "    if len(boxes):\n",
    "        x_left = np.min(boxes[:,0])\n",
    "        width = np.max(boxes[:,0] + boxes[:,2] - x_left)\n",
    "        y_top = np.min(boxes[:,1])\n",
    "        height = np.max(boxes[:,1] + boxes[:,3] - y_top)\n",
    "        \n",
    "        output = np.array([[x_left, y_top, width, height]])\n",
    "\n",
    "    return output\n",
    "\n",
    "def intersection_info(box_i, box_j):\n",
    "    # NMS\n",
    "    x_left_i, y_upper_i, w_i, h_i = box_i\n",
    "    x_right_i = x_left_i + w_i\n",
    "    y_lower_i = y_upper_i + h_i\n",
    "    area_i = w_i*h_i\n",
    "\n",
    "    x_left_j, y_upper_j, w_j, h_j = box_j\n",
    "    x_right_j = x_left_j + w_j\n",
    "    y_lower_j = y_upper_j + h_j\n",
    "    area_j = w_j*h_j\n",
    "\n",
    "    x_left  = max(x_left_i, x_left_j)\n",
    "    y_upper = max(y_upper_i, y_upper_j)\n",
    "\n",
    "    x_right = min(x_right_i, x_right_j)\n",
    "    y_lower = min(y_lower_i, y_lower_j)\n",
    "\n",
    "    w_intersec = max(0, x_right - x_left)\n",
    "    h_intersec = max(0, y_lower - y_upper)\n",
    "\n",
    "    intersection_area = w_intersec*h_intersec\n",
    "    union_area = area_i + area_j - intersection_area\n",
    "    IoU = intersection_area / union_area\n",
    "    \n",
    "    return list([x_left, y_upper, w_intersec, h_intersec, intersection_area, union_area, IoU, area_i, area_j])\n",
    "\n",
    "def compare_predictions_with_truth(pred_boxes, true_boxes):\n",
    "    \n",
    "    intersection_boxes = []\n",
    "    match_found_pred = np.ones(len(pred_boxes))\n",
    "    match_found_true = np.ones(len(true_boxes))\n",
    "    \n",
    "    for i, box_pred in enumerate(pred_boxes):\n",
    "        for j, box_true in enumerate(true_boxes):\n",
    "                     \n",
    "            x_left, y_upper, w_intsc, h_intsc, A_intsc, A_union, IoU, A_pred, A_true  = intersection_info(box_pred, box_true)\n",
    "            \n",
    "            if IoU >= 0.1:\n",
    "                match_found_pred[i] = 0\n",
    "                match_found_true[j] = 0\n",
    "                intersection_boxes = [*intersection_boxes, [x_left, y_upper, w_intsc, h_intsc]]\n",
    "            \n",
    "            elif (IoU >= 0.05) and ((A_intsc>=A_pred) or (A_intsc>=A_true)):\n",
    "                match_found_pred[i] = 0\n",
    "                match_found_true[j] = 0\n",
    "                intersection_boxes = [*intersection_boxes, [x_left, y_upper, w_intsc, h_intsc]]\n",
    "            \n",
    "            \n",
    "    intersection_boxes = np.array(intersection_boxes)\n",
    "        \n",
    "    false_positives = match_found_pred.sum()\n",
    "    false_negatives = match_found_true.sum()\n",
    "    true_positives  = len(match_found_true) - false_negatives\n",
    "                \n",
    "    return intersection_boxes, false_positives, false_negatives, true_positives\n",
    "\n",
    "def boxes_from_mask(mask, area_threshold):\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, 2)\n",
    "    \n",
    "    # Threshold by area\n",
    "    contours_areathrsh = []\n",
    "    for cnt in contours:\n",
    "        if cv2.contourArea(cnt) > area_threshold:\n",
    "            contours_areathrsh = [*contours_areathrsh, cnt]\n",
    "    contours_areathrsh = np.array(contours_areathrsh)\n",
    "    \n",
    "    # combine boxes thresholded\n",
    "    boxes = []\n",
    "    for i, cnt in enumerate(contours_areathrsh):\n",
    "        box = cv2.boundingRect(cnt)\n",
    "        boxes = [*boxes, box]\n",
    "    boxes = np.array(boxes)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "def plot_results(img, mask, pred_boxes, true_boxes, intersection_boxes, title):\n",
    "    colors = ['r', 'c', 'g', 'b', 'y', 'm']\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    fig = plt.figure(figsize=(21,10))\n",
    "\n",
    "    # Plot boxes - Area thresholded\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    for i, box in enumerate(intersection_boxes):\n",
    "        x,y,w,h = box # upper left\n",
    "        rect = patches.Rectangle((x,y), w,h, edgecolor='y', facecolor='y', fill=True, linewidth=2, alpha=0.25)\n",
    "        ax.add_patch(rect)     \n",
    "    \n",
    "    for i, box in enumerate(pred_boxes):\n",
    "        x,y,w,h = box # upper left\n",
    "        rect = patches.Rectangle((x,y), w,h, edgecolor='r', fill=False, linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "    for i, box in enumerate(true_boxes):\n",
    "        x,y,w,h = box # upper left\n",
    "        rect = patches.Rectangle((x,y), w,h, edgecolor='g', fill=False, linewidth=2)\n",
    "        ax.add_patch(rect)           \n",
    "        \n",
    "     \n",
    "    fig.suptitle(title + ' | Found: {}, Truth: {}'.format(len(pred_boxes), len(true_boxes)), fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_edge_detection(threshold_mask, gaussian_mask, img_dir, true_df, object_df, land_df):\n",
    "    \n",
    "    #####################################################\n",
    "    area_threshold = 9\n",
    "    \n",
    "    testObjects = True\n",
    "    testLand = True\n",
    "    testTrue = True\n",
    "    \n",
    "    doPlots = True\n",
    "    #####################################################\n",
    "\n",
    "    files = os.listdir(img_dir)\n",
    "    n_files = len(files)\n",
    "\n",
    "    loops = n_files\n",
    "\n",
    "    object_metrics = {'FPs':0, 'FNs':0, 'TPs': 0}\n",
    "    land_metrics   = {'FPs':0, 'FNs':0, 'TPs': 0}\n",
    "    true_metrics   = {'FPs':0, 'FNs':0, 'TPs': 0}\n",
    "\n",
    "    scenery = ''\n",
    "\n",
    "    for i, file in enumerate(files[:loops]):\n",
    "        true_boxes = []\n",
    "        pred_boxes = []\n",
    "        intersection_boxes = []\n",
    "        FP, FN, TP = 0,0,0\n",
    "\n",
    "        if i>0:\n",
    "\n",
    "            img = np.array(Image.open(os.path.join(img_dir, file)))\n",
    "\n",
    "            isAnomObject = object_df['file'].str.contains(file).sum()\n",
    "            isAnomLand   = land_df['file'].str.contains(file).sum()\n",
    "            isTrue       = true_df['file'].str.contains(file).sum()\n",
    "\n",
    "            ##################### ALGORITHM #####################\n",
    "            mask = edge_detection(img, threshold_mask, gaussian_mask)\n",
    "            #####################################################\n",
    "\n",
    "            if isAnomObject and testObjects:\n",
    "                scenery = 'Object'\n",
    "                true_boxes = object_df[object_df['file'] == file]\n",
    "                true_boxes = true_boxes[['x','y','w','h']].to_numpy()      \n",
    "            elif isAnomLand and testLand:\n",
    "                scenery = 'Land'\n",
    "                true_boxes = land_df[land_df['file'] == file]\n",
    "                true_boxes = true_boxes[['x','y','w','h']].to_numpy()       \n",
    "            elif isTrue and testTrue:\n",
    "                scenery = 'Nothing'\n",
    "            else:\n",
    "                scenery = 'Error'\n",
    "                continue\n",
    "\n",
    "            # Pred Boxes\n",
    "            pred_boxes = boxes_from_mask(mask, area_threshold)\n",
    "\n",
    "            if isAnomLand:\n",
    "                pred_boxes = covering_rectangle_from_square_boxes(pred_boxes) # Combine small boxes\n",
    "\n",
    "            # Calculate accuracy\n",
    "            intersection_boxes, FP, FN, TP = compare_predictions_with_truth(pred_boxes, true_boxes)\n",
    "\n",
    "            # Metrics\n",
    "            if isAnomObject and testObjects: \n",
    "                update = {'FPs':FP, 'FNs':FN, 'TPs': TP}\n",
    "                object_metrics = {i: object_metrics.get(i, 0) + update.get(i, 0) for i in set(update)}\n",
    "            elif isAnomLand and testLand:\n",
    "                update = {'FPs':FP, 'FNs':FN, 'TPs': TP}\n",
    "                land_metrics = {i: land_metrics.get(i, 0) + update.get(i, 0) for i in set(update)}\n",
    "            elif isTrue and testTrue:\n",
    "                update = {'FPs':FP, 'FNs':FN, 'TPs': TP}\n",
    "                true_metrics = {i: true_metrics.get(i, 0) + update.get(i, 0) for i in set(update)}\n",
    "\n",
    "            # Plot\n",
    "            title = 'FP: {}, FN: {}, TP: {} | {}'.format(FP, FN, TP, scenery)\n",
    "\n",
    "            if doPlots:\n",
    "                plot_results(img, mask, pred_boxes, true_boxes, intersection_boxes, title)\n",
    "                \n",
    "            print('{}/{}'.format(int(file.split('.')[0]), n_files), end='\\r')\n",
    "            \n",
    "    print('\\n\\n######## mask threshold: {} | gaussian threshold: {} ########'.format(threshold_mask, gaussian_mask))    \n",
    "    print('Object: {}'.format(object_metrics))\n",
    "    print('Recall: {:.4f}'.format(object_metrics['TPs']/(object_metrics['FNs']+object_metrics['TPs']+0.00000001)))\n",
    "    print('Precision: {:.4f}'.format(object_metrics['TPs']/(object_metrics['FPs']+object_metrics['TPs']+0.00000001)))\n",
    "    print('--------')\n",
    "    print('Land: {}'.format(land_metrics))\n",
    "    print('Recall: {:.4f}'.format(land_metrics['TPs']/(land_metrics['FNs']+land_metrics['TPs']+0.00000001)))\n",
    "    print('Precision: {:.4f}'.format(land_metrics['TPs']/(land_metrics['FPs']+land_metrics['TPs']+0.00000001)))\n",
    "    print('--------')\n",
    "    print('True: {}'.format(true_metrics))\n",
    "    print('Recall: {:.4f}'.format(true_metrics['TPs']/(true_metrics['FNs']+true_metrics['TPs']+0.00000001)))\n",
    "    print('Precision: {:.4f}'.format(true_metrics['TPs']/(true_metrics['FPs']+true_metrics['TPs']+0.00000001)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haak - Flight 1 - Train / Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/Volumes/Samsung_T5/Haak_data/Flight1/20170926_norm_1100-3900/'\n",
    "#thresholds_masks = [2, 5, 10, 15, 20, 25]\n",
    "thresholds_masks = [4]\n",
    "gaussian_masks  = [7]\n",
    "\n",
    "for threshold_mask in thresholds_masks:\n",
    "    for gaussian_mask in gaussian_masks:\n",
    "        run_edge_detection(threshold_mask, gaussian_mask, img_dir, \n",
    "                 Haak1_true_train, Haak1_ano_object_train, Haak1_ano_land_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Haak1_true_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4b6eaef2e80b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m test_sets = [[Haak1_img_dir,  Haak1_true_test,  Haak1_ano_object_test,  Haak1_ano_land_test],\n\u001b[0m\u001b[1;32m      9\u001b[0m              \u001b[0;34m[\u001b[0m\u001b[0mHaak3_img_dir\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mHaak3_true_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mHaak3_ano_object_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mHaak3_ano_land_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m              [Danilo_img_dir, Danilo_true_test, Danilo_ano_land_test,   empty]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Haak1_true_test' is not defined"
     ]
    }
   ],
   "source": [
    "Haak1_img_dir = '/Volumes/Samsung_T5/Haak_data/Flight1/20170926_norm_1100-3900/'\n",
    "Haak3_img_dir = '/Volumes/Samsung_T5/Haak_data/Flight3/20170926_norm_1100-3900/'\n",
    "Danilo_img_dir = '/Volumes/Samsung_T5/Danilo_data/IR/All-Frames-Simple/'\n",
    "n_component = 3\n",
    "threshold_mask = 5\n",
    "\n",
    "\n",
    "test_sets = [[Haak1_img_dir,  Haak1_true_test,  Haak1_ano_object_test,  Haak1_ano_land_test],\n",
    "             [Haak3_img_dir,  Haak3_true_test,  Haak3_ano_object_test,  Haak3_ano_land_test],\n",
    "             [Danilo_img_dir, Danilo_true_test, Danilo_ano_land_test,   empty]]\n",
    "\n",
    "# Train with all train data - aggregate to numpy 3D object\n",
    "train_data = Haak1_true_train[:5000]\n",
    "train_imgs_3D = []\n",
    "for file in train_data['file']:\n",
    "    img = np.array(Image.open(os.path.join(Haak1_img_dir, file)))\n",
    "    train_imgs_3D = [*train_imgs_3D, img]\n",
    "train_imgs_3D = np.array(train_imgs_3D)\n",
    "\n",
    "# Test\n",
    "for i, (img_dir, true_test, object_test, land_test) in enumerate(test_sets):\n",
    "    print('##### Test set: {} #####'.format(i+1))\n",
    "    \n",
    "    PCA_model = PCA(n_components = n_component).fit(train_imgs_3D)\n",
    "\n",
    "    run_PCA(PCA_model, threshold_mask, img_dir,\n",
    "                 Haak1_true_validate, Haak1_ano_object_train, Haak1_ano_land_train)\n",
    "\n",
    "train_imgs_3D = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
